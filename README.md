# DuckNet-Attention

Triá»ƒn khai kiáº¿n trÃºc DuckNet tÃ­ch há»£p cÆ¡ cháº¿ attention báº±ng PyTorch cho phÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh y táº¿, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho phÃ¢n Ä‘oáº¡n polyp Ä‘Æ°á»ng tiÃªu hÃ³a.

## ğŸ“‹ Tá»•ng quan

Dá»± Ã¡n nÃ y triá»ƒn khai DuckNet-Attention, má»™t phiÃªn báº£n cáº£i tiáº¿n cá»§a kiáº¿n trÃºc DuckNet cÃ³ tÃ­ch há»£p cÆ¡ cháº¿ attention Ä‘á»ƒ tÄƒng cÆ°á»ng biá»ƒu diá»…n Ä‘áº·c trÆ°ng vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t phÃ¢n Ä‘oáº¡n. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ trÃªn bá»™ dá»¯ liá»‡u Kvasir-SEG cho bÃ i toÃ¡n phÃ¢n Ä‘oáº¡n polyp trong hÃ¬nh áº£nh ná»™i soi Ä‘áº¡i trÃ ng.

## ğŸ—ï¸ Kiáº¿n trÃºc

### Äáº·c Ä‘iá»ƒm cá»§a DuckNet-Attention:
- **Kiáº¿n trÃºc Encoder-Decoder**: Cáº¥u trÃºc giá»‘ng U-Net vá»›i skip connections
- **DuckBlocks**: Khá»‘i xÃ¢y dá»±ng cá»‘t lÃµi sá»­ dá»¥ng separable convolutions
- **CÆ¡ cháº¿ Attention**: 
  - Self-attention á»Ÿ lá»›p bottleneck
  - Attention gates á»Ÿ skip connections cá»§a decoder
  - Channel vÃ  spatial attention trong DuckBlocks
- **Fusion Ä‘a tá»· lá»‡**: Káº¿t ná»‘i ResPath Ä‘á»ƒ cáº£i thiá»‡n gradient flow

### CÃ¡c thÃ nh pháº§n chÃ­nh:
- **SeparableConv2d**: Convolution tÃ¡ch biá»‡t theo chiá»u sÃ¢u Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t
- **DuckBlock**: Khá»‘i residual cáº£i tiáº¿n vá»›i attention tÃ¹y chá»n
- **AttentionGate**: CÆ¡ cháº¿ cá»•ng cho skip connections
- **ResidualBlock**: Káº¿t ná»‘i residual chuáº©n á»Ÿ bottleneck

## ğŸ“Š Hiá»‡u suáº¥t

MÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u suáº¥t xuáº¥t sáº¯c trÃªn bá»™ dá»¯ liá»‡u Kvasir-SEG:

| Äá»™ Ä‘o | DuckNet-Attention | DuckNet thÆ°á»ng |
|-------|-------------------|----------------|
| **Dice Score** | 0.9422 Â± 0.0329 | Baseline |
| **IoU Score** | 0.9289 Â± 0.0580 | Baseline |
| **Äá»™ chÃ­nh xÃ¡c** | 0.9864 Â± 0.0147 | Baseline |
| **Precision** | 0.9825 Â± 0.0075 | Baseline |
| **Recall** | 0.9444 Â± 0.0577 | Baseline |

## ğŸ—‚ï¸ Bá»™ dá»¯ liá»‡u

**Bá»™ dá»¯ liá»‡u Kvasir-SEG**: Táº­p há»£p cÃ¡c hÃ¬nh áº£nh Ä‘Æ°á»ng tiÃªu hÃ³a vá»›i mask phÃ¢n Ä‘oáº¡n polyp tÆ°Æ¡ng á»©ng, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh phÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh y táº¿.

- **Nhiá»‡m vá»¥**: PhÃ¢n Ä‘oáº¡n nhá»‹ phÃ¢n (polyp vs ná»n)
- **KÃ­ch thÆ°á»›c áº£nh**: 256Ã—256 pixels
- **Äá»‹nh dáº¡ng**: áº¢nh RGB vá»›i mask nhá»‹ phÃ¢n

## ğŸ› ï¸ CÃ i Ä‘áº·t

```bash
# Clone repository
git clone https://github.com/yourusername/DuckNet-Attention.git
cd DuckNet-Attention

# CÃ i Ä‘áº·t cÃ¡c package cáº§n thiáº¿t
pip install torch torchvision
pip install numpy pandas matplotlib
pip install opencv-python pillow
pip install scikit-learn
```

## ğŸ“– Sá»­ dá»¥ng

### Huáº¥n luyá»‡n mÃ´ hÃ¬nh

Code huáº¥n luyá»‡n chÃ­nh Ä‘Æ°á»£c chá»©a trong Jupyter notebook `train-ducknet-att.ipynb`. Notebook bao gá»“m:

1. **Tiá»n xá»­ lÃ½ vÃ  tÄƒng cÆ°á»ng dá»¯ liá»‡u**
2. **Triá»ƒn khai kiáº¿n trÃºc mÃ´ hÃ¬nh**
3. **VÃ²ng láº·p huáº¥n luyá»‡n vá»›i validation**
4. **ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t vÃ  trá»±c quan hÃ³a**
5. **So sÃ¡nh mÃ´ hÃ¬nh (cÃ³/khÃ´ng cÃ³ attention)**

### TÃ­nh nÄƒng chÃ­nh trong Notebook:

- **Load dá»¯ liá»‡u**: Class dataset tÃ¹y chá»‰nh cho Kvasir-SEG
- **CÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh**: Cáº£ DuckNet thÆ°á»ng vÃ  DuckNet-Attention
- **Cáº¥u hÃ¬nh huáº¥n luyá»‡n**: 
  - Optimizer: AdamW
  - Loss Function: Káº¿t há»£p Dice + BCE Loss
  - Learning Rate Scheduling
  - Early Stopping vá»›i model checkpointing

### Cháº¡y huáº¥n luyá»‡n:

```python
# Load notebook vÃ  cháº¡y táº¥t cáº£ cells
jupyter notebook train-ducknet-att.ipynb
```

## ğŸ§  Chi tiáº¿t kiáº¿n trÃºc mÃ´ hÃ¬nh

### Cáº¥u trÃºc DuckBlock:
```
Input â†’ SeparableConv2d â†’ BatchNorm â†’ ReLU â†’ 
        SeparableConv2d â†’ BatchNorm â†’ Attention (tÃ¹y chá»n) â†’ 
        Residual Connection â†’ Output
```

### CÆ¡ cháº¿ Attention:
- **Self-Attention**: Ãp dá»¥ng á»Ÿ lá»›p bottleneck cho ngá»¯ cáº£nh toÃ n cá»¥c
- **Attention Gates**: Lá»c cÃ¡c Ä‘áº·c trÆ°ng liÃªn quan trong skip connections
- **Channel Attention**: Táº­p trung vÃ o cÃ¡c kÃªnh Ä‘áº·c trÆ°ng quan trá»ng
- **Spatial Attention**: Nháº¥n máº¡nh cÃ¡c vá»‹ trÃ­ khÃ´ng gian liÃªn quan

## ğŸ“ˆ Trá»±c quan hÃ³a káº¿t quáº£

Notebook bao gá»“m trá»±c quan hÃ³a toÃ n diá»‡n:
- ÄÆ°á»ng cong loss huáº¥n luyá»‡n/validation
- Tiáº¿n trÃ¬nh Dice score
- Dá»± Ä‘oÃ¡n máº«u so vá»›i ground truth
- Trá»±c quan hÃ³a attention maps
- PhÃ¢n tÃ­ch so sÃ¡nh giá»¯a cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh

## ğŸ”¬ Äá»™ Ä‘o Ä‘Ã¡nh giÃ¡

- **Há»‡ sá»‘ Dice**: Äo lÆ°á»ng Ä‘á»™ chá»“ng láº¥p giá»¯a dá»± Ä‘oÃ¡n vÃ  ground truth
- **IoU (Intersection over Union)**: Chá»‰ sá»‘ Jaccard cho cháº¥t lÆ°á»£ng phÃ¢n Ä‘oáº¡n
- **Äá»™ chÃ­nh xÃ¡c Pixel**: Äá»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i tá»•ng thá»ƒ theo pixel
- **Precision**: Tá»· lá»‡ true positive
- **Recall**: Äá»™ Ä‘o sensitivity

## ğŸ“ TrÃ­ch dáº«n

Náº¿u báº¡n sá»­ dá»¥ng cÃ´ng trÃ¬nh nÃ y trong nghiÃªn cá»©u, vui lÃ²ng trÃ­ch dáº«n:

```bibtex
@article{ducknet_attention,
  title={DuckNet-Attention: Cáº£i tiáº¿n phÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh y táº¿ vá»›i cÆ¡ cháº¿ Attention},
  author={TÃªn cá»§a báº¡n},
  journal={Táº¡p chÃ­ cá»§a báº¡n},
  year={2024}
}
```

## ğŸ¤ ÄÃ³ng gÃ³p

ChÃºng tÃ´i hoan nghÃªnh cÃ¡c Ä‘Ã³ng gÃ³p! Vui lÃ²ng táº¡o Pull Request.

## ğŸ“„ Giáº¥y phÃ©p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo MIT License - xem file LICENSE Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ™ Lá»i cáº£m Æ¡n

- Cáº£m Æ¡n kiáº¿n trÃºc DuckNet gá»‘c Ä‘Ã£ truyá»n cáº£m há»©ng
- Cáº£m Æ¡n nhá»¯ng ngÆ°á»i Ä‘Ã³ng gÃ³p cho bá»™ dá»¯ liá»‡u Kvasir-SEG
- Cáº£m Æ¡n cá»™ng Ä‘á»“ng PyTorch vÃ¬ tÃ i liá»‡u vÃ  cÃ´ng cá»¥ tuyá»‡t vá»i